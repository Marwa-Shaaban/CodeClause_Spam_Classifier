#importing main libraries
import numpy as np
import pandas as pd
import string
import nltk
from nltk.corpus import stopwords


#loading the dataset
from google.colab import files
up=files.upload()

#show the head of dataset
df=pd.read_csv("emails.csv")
df.head()


#show the shape
df.shape

#show numbers of columns
df.columns

#remove duplicates
df.drop_duplicates(inplace=True)

df.shape

#show number of missing (NAN ,nAn ,na) data for each column
df.isnull().sum()

#download the stopwords packages
nltk.download('stopwords')

def process_text(text):

  #1 remove punctuation
  #2 remove stopwords
  #3 return a list of clean text words

  #1
  nopunc=[char for char in text if char not in string.punctuation]
  nopunc=''.join(nopunc)

  #2
  clean_words=[word for word in nopunc.split() if word.lower() not in stopwords.words('english') ]

  #3
  return clean_words

#show the tokenization ( a list of tokens also called lemmas )
df['text'].head().apply(process_text)

#exaple
message4 ="hello world hello hello world play"
message5="test test test test one hello"
print(message4)
print()

#convert the text to a matrix of token counts
from sklearn.feature_extraction.text import CountVectorizer
bow4= CountVectorizer(analyzer=process_text).fit_transform( [[message4] ,[message5]])
print(bow4)
print()
bow4.shape
    


#covert a collection of text to a matiz tokens
from sklearn.feature_extraction.text import CountVectorizer
messages_bow =CountVectorizer(analyzer=process_text).fit_transform(df['text'])

#split the data into 80% training and 20% testing
from sklearn.model_selection import train_test_split
X_train , X_test , Y_train , Y_test =train_test_split(messages_bow ,    df['spam']    ,     test_size=0.20    ,random_state=0 )


#get the shape of messages_bow
messages_bow.shape

#create and train the naive bayes classifier
from sklearn.naive_bayes import MultinomialNB
classifier=MultinomialNB().fit( X_train  , Y_train)

#print the predictions
print(classifier.predict(X_train))

#print the actual values
print(Y_train.values)

#evaluate the model on the training data set 
from sklearn.metrics import classification_report , confusion_matrix , accuracy_score
pred=classifier.predict(X_train)
print(classification_report(Y_train , pred) )
print()
print('Confusion metrix : \n' , confusion_matrix(Y_train , pred))
print()
print('accuracy' , accuracy_score(Y_train ,pred))
    


#print the predictions
print(classifier.predict(X_test))

#print the actual values
print(Y_test.values)

#evaluate the model on the taining data set
from sklearn.metrics import classification_report , confusion_matrix , accuracy_score
pred=classifier.predict(X_test)
print(classification_report(Y_test ,pred) )
print()
print('confusion metrics :\n' ,confusion_matrix(Y_test ,pred))
print()
print('accuracy :' , accuracy_score(Y_test ,pred)) 






